
= Main =

== What to do? ==

=== Ideal shape of Samsung's CXL solutions ===

{| class="wikitable sortable mw-collapsible"
|+ 
! category
! name and description
!use cases
!requirements
! notes
|-
| HW: Leaf node
|
|
|
|
|-
| HW: Switch
|
|
|
|
|-
| HW: Host CPU
|
|
|
|
|-
| SW: OS
|
|
|
|
|-
| SW: Applications
|
|
|
|
|-
|}

== Why? ==

== How? ==


= Appendix =

== Use cases ==

What are the killer use cases of CXL solutions (memory/storage)? Use case definition should be the first of all. Because it is the source of all the functional and non-functional requirements of the system.
{| class="wikitable sortable mw-collapsible"
|+
! Use cases
! Features
! Ideal Case
!What 
|-
| rowspan="9" | Memory Pool
| Tiered
| 
|
|-
|Shared by Multi-Host
|
|
|-
|Topology
|
|
|-
|Hot Plugging/Unplugging (changes capacity)
|
|
|-
|Hot Swap (no change in capacity)
|
|
|-
|P2P data path
|
|
|-
|CXL-memory for application data
|
|
|-
|CXL-memory for kernel data
|
|
|-
|Computational
|
|
|}


#memory pool
##tiered
###media
####DRAM only? (1st tier is DIMM DRAM, and the 2nd tier is CXL-DRAM)
####DRAM+NAND hybrid? (1st tier is DIMM DRAM, and the 2nd tier is CXL-SSD<ref>DRAM works as a fast tier which has pre-fetched data from NAND, thanks to host's explicit pre-fetching hint</ref>)
###topology
####nested CXL switches?
##shared
##dynamtically-extendable by (sudden/scheduled) plugging/unplugging
##for application data
##for kernel data? (very dangerous)
##computational? (does it really make any sense?)

==CXL business SWOT analysis==

Definition: what is the success criteria of Samsung's CXL business?

=== Features (CXL 3.0) ===

Highlights of the CXL 3.0 spec. [https://www.computeexpresslink.org/_files/ugd/0c1418_1798ce97c1e6438fba818d760905e43a.pdf CXL Revision 3.0, Version 1.0 (August 1, 2022)]

# fabric capabilities <ref group="cxl_feat">CXL fabric non-tree topology</ref>
## multi-headed and fabric attached devices
## enhanced fabric management
## composable disaggregated infrastructure <ref group="cxl_feat">what is exactly this?</ref>
# better scalability and improved resource utilization
## enhanced memory pooling
## multi-level switching
## new enhanced coherency capabilities
## improved software capabilities
# performance
## doubled the bandwidth to 64GT/s
## zero added latency over CXL 2.0
# compatibility
## full backward compatibility with CXL 2.0, CXL 1.1, and CXL 1.0 
# (enhanced) coherency
## back invalidation<ref group="cxl_feat">why did CXL introduce this feature in the CXL 3.0 spec? To support real peer-to-peer communication, in the non-tree topology? To support memory sharing, rather just by supporting memory pool?</ref> of host's caches, for type 2 devices (accelerators with memory)
# peer-to-peer communication



----
<references group="cxl_feat"/>

=== Strengths ===
: internal, positive attributes of you. These are things that are within your control.

#Scalability of memory


=== Weaknesses ===
: negative factors that detract from your strengths. These are things that you might need to improve on to be competitive.

#Latency
##Long average latency (added by CXL switches)
### at least 99ns for one hop (Xconn said in FMS 2023): so the round-trip latency may be about 360ns to get a data from CXL-DRAM via CXL switch
###*host -> CXL_Switch (100ns: based on Xconn's comment) -> CXL_DRAM (160ns: asumming that CTRL 80ns + DRAM 80ns) -> CXL_Switch (100ns: based on Xconn's comment).
###some other guy said it is at least 200ns<ref> URL here </ref>
###Can host CPU hardware or system software (such as GFS-II) help us avoid or mitigate this longer latency impact?
##Degraded tail latency characteristics (non-deterministic latency to access CXL-DRAM, compared to DIMM)
###What about the many-host case? e.g., multiple (N) compute hosts are connected to one memory pool which is comprised of multiple (M) CXL devices
###Non-deterministic latency problems caused by many-tenant should be considered and handled
#Bandwidth limitation (caused by underlying PCIe)
##Then, what will happen if we change it to NVLink? Will it solve all the bandwidth related problems?
##Let's consider also the bandwidth bottleneck issue of upstream port of CXL switch


=== Opportunities ===
: external factors in your business environment that are likely to contribute to your success

# demand for large amount of memory

#Can we re-shape the system environment so that it can be more friendly to the CXL-based memory pool, in terms of the latency?
##First, CPU
### Do we need to change the behavior of the hardware such as host CPUs so that the CPU can be utilized 100%, without any idle time, even there exist many memory waits caused by longer memory response time? (e.g., It would be good if there was a CPU feature to make context switching very efficient, so that there is no need to get help from the software, such as OS kernel)
####This is possible scenario if CPU can switch contexts very efficiently/shortly to avoid idle time. The point is, the context switching should be very efficient, because the idle time caused by CXL memory pool may be tens of nano seconds. So if the context switching overhead is not small in terms of cycles (and time), then this can not be the solution to hide the long latency.
###Second, system software
#### Is it possible to save the CPU from the long latency impact, by
###Third, application software
#Threats: external factors that you have no control over. You may want to consider putting in place contingency plans for dealing with them if they occur.
##CXL ecosystem
###Host software (system + application)
###Switch
###CXL fabric management
##Alternative technologies for wider bandwidth and lower latency
###NVIDIA NVLink
###What else?


== Keywords for ideation ==

: '''<span style="color:red">why</span>'''? what is the background of introducing the following items into 3.0?

* memory sharing vs memory pooling


=== remarkable things introduced in CXL 3.0 ===
 
* 256B flit mode
* back-invalidate
* multiple CXL.cache devices per VCS
* fabric
* memory sharing (v3.0)
* memory pooling (v2.0)
* intra-VH P2P using UIO





=== Terminilogies ===

{| class="wikitable sortable mw-collapse"
|+
! Terminology
! Definition
|-
| VCS
| Virtual CXL Switch. Includes entities within the physical switch belonging to a single VH. It is identified using the VCS ID
|-
| VH 
| Virtual Hierarchy. Everything from the CXL RP down, including the CXL RP, CXL PPBs, and CXL Endpoints. Hierarchy ID means the same as PCIe
|-
| RP
| Root Port
|-
| PPB
| PCI*-to-PCI Bridge inside a CXL switch that is FM-owned. The port connected to a PPB can be disconnected, or connected to a PCIe component or connected to a CXL component
|-
| FM
| The Fabric Manager is an entity separate from the switch or host firmware that controls aspects of the 0system related to binding and management of pooled ports and devices.
|-
| Flit
| Link Layer Unit of Transfer
|-
| FC
| Flow Control
|-
| UIO
| Unordered Input/Output
|-
| UIE
| Uncorrectable Internal Error
|-
| VA
| Virtual Address
|-
| PA
| Physical Address
|-
| PDM
| Private Device Memory. Device-attached memory that is not mapped to system address space or directly accessible to Host as cacheable memory. Memory located on PCIe devices is of this type. Memory located on a CXL device can be mapped as either PDM or HDM.
|-
| HDM
| Host-managed Device Memory. Device-attached memory that is mapped to system coherent address space and accessible to the Host using standard write-back semantics. Memory located on a CXL device can be mapped as either HDM or PDM.
|-
| MLD
| Multi-Logical Device. CXL component that contains multiple LDs, out of which one LD is reserved for configuration via the FM API, and each remaining LD is suitable for assignment to a different host. Currently MLDs are architected only for Type 3 LDs.
|-
| LD
| Logical Device. Entity that represents a CXL Endpoint that is bound to a VCS. An SLD device contains one LD. An MLD contains multiple LDs.
|-
| FAM
| Fabric-Attached Memory. HDM within a Type 2 or Type 3 device that can be made accessible to multiple hosts concurrently. Each HDM region can either be pooled (dedicated to a single host) or shared (accessible concurrently by multiple hosts).
|-

|-
|}

== View points ==



=== CXL ecosystems: software ===

*Memory management in the Linux kernel
**problems of using


;Adoption barrier
:What blocks the adoption of CXL, in addition to the weakness of the CXL itself?
#limitations of CXL itself

#bandwidth of underlying PCIe? (then, for example, do we need to change it to NVLink one?)
#latency (and degraded tail latency characteristics also)?
#host CPU (hardware perspective)?
#host system software (such as Linux kernel)?
#host application software?


;Ideal shape of CXL
#What is the characteristics of the well-matched system environment with CXL technology?
##Environment which is not that much sensitive to the latency (i.e., latency-tolerance) 
###e.g., many-VM environment? to mitigate the waiting time from the host CPU


;Benefit from CXL
#What innovation can we make in the world of CXL?
##from the perspective of:
###application?
###compute server
###memory and memory pool
###storage
###networking
##Would it be possible that
###CXL changes the impossible to the possible one
###CXL enhances the legacy applications/systems

;Adoption
#Will the world adopt CXL as a default peripheral bus (rather than just having PCIe with no CXL)?
## If yes is the answer, when will be the time? and what event will be the key triggering point?
#What will be the killer application of CXL?
##<s>current CXL (CXL 2.0)</s>
##CXL 3.0
##and future CXL (of what shape and characteristics)?

==Prerequisites (and possible sources) to answer to the key questions==

#Technologies forecasting (5 years later or 10 years later)
#Industry trend regarding CXL from the viewpoint of
##Customers
###Data center players (Hyperscalers)
###others? edge such as Auto/Mobile/...? does it make any sense?
##Competitors
##Partners
##Startups

=References=

==Footnotes automatically generated==

<references />

==References (Manually Added)==


* CXL 3.0
** [https://www.anandtech.com/show/17520/compute-express-link-cxl-30-announced-doubled-speeds-and-flexible-fabrics CXL 3.0 announced: doubled speeds and flexible fabrics] Aug 2, 2022
** [https://www.computeexpresslink.org/_files/ugd/0c1418_a859d94c29684b44b91c458d5dae454e.pdf CXL 3.0 Press Release] CXL Consortium releases Compute Express Link 3.0 specification to expand fabric capabilities and management
** [https://www.computeexpresslink.org/_files/ugd/0c1418_a8713008916044ae9604405d10a7773b.pdf Compute Express Link 3.0 White Paper]


* [https://www.cs.cornell.edu/courses/cs3410/2013sp/lecture/18-caches3-w.pdf Caches (Writing)] Computer Science, Cornell University
* [https://my.eng.utah.edu/~cs7810/pres/09-7810-02.pdf Lecture 2: Snooping-Based Coherence]


*[https://docs.kernel.org/admin-guide/mm/memory-hotplug.html Memory Hot(Un)Plug] Linux kernel documentation
*[https://www.kernel.org/doc/html/v5.6/admin-guide/mm/numa_memory_policy.html#numa-memory-policy NUMA memory policy] Linux kernel documentation
*[https://www.kernel.org/doc/html/v5.6/vm/numa.html What is NUMA?] Linux kernel documentation
*[https://www.kernel.org/doc/html/v5.6/vm/hmm.html Heterogeneous Memory Management (HMM)] Linux kernel documentation
*[https://www.kernel.org/doc/html/v5.0/vm/hmm.html Heterogeneous Memory Management (HMM)] Linux kernel documentation
*[[LWN article: Memory-management changes for CXL]]
*[https://lwn.net/Articles/lsfmmbpf2023/ The 2023 LSFMM+BPF Summit] The 2023 Linux Storage, Filesystem, Memory-Management, and BPF Summit was held May 8 to 10 in Vancouver
*[https://lwn.net/Articles/931416/ LWN 931416 Memory-management changes for CXL]
*[https://lwn.net/Articles/931282/ A storage standards update at LSFMM+BPF]
*[https://lwn.net/Articles/931528/ Live migration of virtual machines over CXL]
*[https://lwn.net/Articles/931421/ The future of memory tiering]
*[https://www.anandtech.com/show/17047/the-intel-12th-gen-core-i912900k-review-hybrid-performance-brings-hybrid-complexity/12 The Intel 12th Gen Core i9-12900K Review: Hybrid Performance Brings Hybrid Complexity] by Dr. Ian Cutress & Andrei Frumusanu on November 4, 2021 9:00 AM EST
*[https://stackoverflow.com/questions/11223523/what-is-the-meaning-of-gfp-in-kmalloc-flags What is the meaning of GFP in kmalloc flags?]
*[https://cseweb.ucsd.edu/classes/fa12/cse260-b/Lectures/Lec17.pdf NUMA architecture and programming]
*[https://www.intel.com/content/www/us/en/developer/articles/technical/memory-performance-in-a-nutshell.html Memory Performance in a Nutshell] Intel; June 6, 2016;
*[https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html Intel® Memory Latency Checker v3.10] Intel

----
References for the question:
*what is the future of CXL? Is CXL-based tiered memory pool the answer?
*what is the current status, what is the expected future, what are the killer use cases?

;DRAM
*[https://compas.cs.stonybrook.edu/~nhonarmand/courses/sp15/cse502/res/dramop.pdf Understanding DRAM Operation] IBM
*[https://trts1004.tistory.com/12109180 SDRAM parameters] tistory.com
*[https://www.gamersnexus.net/guides/3333-memory-timings-defined-cas-latency-trcd-trp-tras What Are Memory Timings? CAS Latency, tRCD, tRP, & tRAS (Pt 1)] Gamers Nexus
*[https://www.anandtech.com/show/3851/everything-you-always-wanted-to-know-about-sdram-memory-but-were-afraid-to-ask/5 Everything You Always Wanted to Know About SDRAM (Memory): But Were Afraid to Ask] ANANDTech
*[https://www.anandtech.com/show/16143/insights-into-ddr5-subtimings-and-latencies Insights into DDR5 Sub-timings and Latencies] ANANDTech

;CXL
*[https://www.synopsys.com/blogs/chip-design/cxl-protocol-memory-pooling.html?utm_source=google&utm_medium=search&utm_term=&utm_campaign=&cmp=ps-SIG-G_S_TestCampaign-PMax How CXL Is Improving Latency in High-Performance Computing] Synopsys
*[https://www.micron.com/solutions/server/cxl CZ120 memory expansion module - Memory capacity for the data center using the CXL™ standard] Micron
*[https://www.spiceworks.com/tech/hardware/guest-article/boosting-ai-ml-performance-with-compute-express-link/ How Compute Express Link (CXL) Can Boost Your AI/ML Performance] spiceworks.com; July 5, 2023;
*[https://www.nextplatform.com/2022/12/05/just-how-bad-is-cxl-memory-latency/#:~:text=Most%20CXL%20memory%20controllers%20add,from%20the%20CPU%2C%20Tavallaei%20explains. JUST HOW BAD IS CXL MEMORY LATENCY?] The Next Platform; December 5, 2022;
*[https://arxiv.org/pdf/2206.02878.pdf TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory] NVIDIA, Meta, UMich

;Intel CPU
*[https://www.intel.com/content/www/us/en/developer/articles/guide/xeon-performance-tuning-and-solution-guides.html Tuning Guides for Intel® Xeon® Scalable Processor-Based Systems] Intel.com; April 23, 2023;

