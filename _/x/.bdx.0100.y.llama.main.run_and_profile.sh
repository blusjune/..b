#!/bin/bash
# .bdx.0100.y.llama.main.run_and_profile.sh
# 20230910_225422




set -x;
_ts="date +%Y%m%d_%H%M%S";
_tstamp="$($_ts)";
_tmp_bdx_d=".tmp.bdx.d";
mkdir -p $_tmp_bdx_d;




###
### Check prerequisites: execution environment (venv, torch-for-cpu)
###
echo "### Let me check where 'torchrun' is:  '$(which torchrun)'";   ### Litmus test to check the virtual environment
read -p "### Do we need to (re-)install all things required? [y|N] " _answer;
if [ "X$_answer" = "Xy" ]; then
	python -m venv .env;
	source .env/bin/activate; ### must do
	python -m pip install --upgrade pip;
	python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu;
	python -m pip install gprof2dot; ### python profiling and visualization
	python -m pip install snakeviz; ### python profiling and visualization
	python -m pip install viztracer; ### python profiling and visualization
	#python -m pip install pycscope;
	#python -m pip install pycallgraph; ### python profiling and visualization ### ERROR?
	python -m pip install -e .;
	if [ "X$(basename $(which eog))" != "Xeog" ]; then
		sudo apt install eog;
	fi
	echo $_tstamp > ${_tmp_bdx_d}/.flag.venv.created;
	echo $_tstamp > ${_tmp_bdx_d}/.flag.pkg.installed;
else
	source .env/bin/activate; ### must do
fi

###
### Check prerequisites: good download of tokenizer.model
###
if [ -f ${_tmp_bdx_d}/.flag.venv.created -a -f ${_tmp_bdx_d}/.flag.pkg.installed ]; then
	echo "### GOOD: execution environment (as a prerequisite) is ready";
	_chksum_tokenizer_ref="eeec4125e9c7560836b4873b6f8e3025";
	_chksum_tokenizer_test="$( cat tokenizer.model | md5sum | awk '{ print $1 }' )";
	if [ "X$_chksum_tokenizer_ref" = "X$_chksum_tokenizer_test" ]; then
		echo "### GOOD: tokenizer.model checksum matches the reference value";
	else
		echo "### ERROR: tokenizer.model checksum does not match -- EXIT 18";
		exit 18;
	fi
else
	echo "### ERROR: execution environment is not ready -- EXIT 18";
	exit 18;
fi




###
### parameters to control the test
###
_exec_count="0";
_xprmnt_sn="$(printf '%08d' $_exec_count)";
_ckpt_dir="llama-2-7b/";	# "llama-2-7b/"
_nproc_per_node="1";		# 1
_max_seq_len="256";		# 128
_max_gen_len="64";		# 64
_max_batch_size="1";		# 1
_xprmnt_ai_model="LLAMA2-7B"; ### later, include tensor (model) parallelism also
_xprmnt_cpu_model="CORE-I7-1270P"; ### XEON-{PL/GO/SI/BR}-dddd
_xprmnt_name="${_xprmnt_sn}_${_xprmnt_ai_model}_${_xprmnt_cpu_model}_P${_nproc_per_node}_S${_max_seq_len}_G${_max_gen_len}_B${_max_batch_size}";
_xprmnt_uuid="${_tstamp}.${_xprmnt_name}";
export STAR_DOE_CONF__LLAMA2__NPROC_PER_NODE=${_nproc_per_node};
export STAR_DOE_CONF__LLAMA2__MAX_SEQ_LEN=${_max_seq_len};
export STAR_DOE_CONF__LLAMA2__MAX_GEN_LEN=${_max_gen_len};
export STAR_DOE_CONF__LLAMA2__MAX_BATCH_SIZE=${_max_batch_size};
export STAR_DOE_CONF__XPRMNT_TIMESTAMP="${_tstamp}";
export STAR_DOE_CONF__XPRMNT_NAME="${_xprmnt_name}";




###
### STAR: profiling
###
### https://docs.python.org/3/library/profile.html
### https://medium.com/@narenandu/profiling-and-visualization-tools-in-python-89a46f578989
### https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html
###
_tmp_star_d=".tmp.star.d";
mkdir -p $_tmp_star_d;
_star_prof_out_bin=".star.${_xprmnt_uuid}.prof_out.bin"; # generated by (prof).dump_stats('.star.prof_out.bin') or python -m cProfile -o test.pstats test.py
_star_prof_out_txt=".star.${_xprmnt_uuid}.prof_out.txt"; # generated by (pstats).print_stats()
_star_prof_out_svg=".star.${_xprmnt_uuid}.prof_out.svg"; # generated by gprof2dot
_viztr_prof_out_json=".star.${_xprmnt_uuid}.viztracer_out.json"; # generated by viztracer
_viztr_tracer_entries="100000000";




###
### functions
###

function main_exec_profiling_with_cprofile()
{
	echo "### =============================================";
	echo "### function main_exec_profiling_with_cprofile()"; 
	_main_python_file_with_cprofile="example_text_completion.star.cprofile.py";   ### _main_python_file="example_text_completion.py";
	torchrun --nproc_per_node $_nproc_per_node $_main_python_file_with_cprofile --ckpt_dir $_ckpt_dir  --tokenizer_path tokenizer.model --max_seq_len $_max_seq_len --max_batch_size $_max_batch_size ;
	( cd $_tmp_star_d; 
	gprof2dot -f pstats $_star_prof_out_bin | dot -Tsvg -o $_star_prof_out_svg;
	eog $_star_prof_out_svg &
	snakeviz $_star_prof_out_bin &
	#read -p "### Do you want to see ${_star_prof_out_svg} via eog? [Y|n] " _answer;
	#if [ "X$_answer" = "Xy" ]; then
	#	eog $_star_prof_out_svg &
	#fi
	#read -p "### Do you want to see ${_star_prof_out_bin} via snakeviz? [Y|n] " _answer;
	#if [ "X$_answer" = "Xy" ]; then
	#	snakeviz $_star_prof_out_bin &
	#fi
	)
}

function main_exec_profiling_with_viztracer()
{
	echo "### =============================================";
	echo "### function main_exec_profiling_with_viztracer()"; 
	_main_python_file_with_viztracer="example_text_completion.star.viztracer.py";   ### _main_python_file="example_text_completion.py";
	# python -m viztracer --tracer_entries $_viztr_tracer_entries -o ${_tmp_star_d}/${_viztr_prof_out_json} \
	torchrun --nproc_per_node $_nproc_per_node $_main_python_file_with_viztracer --ckpt_dir $_ckpt_dir  --tokenizer_path tokenizer.model --max_seq_len $_max_seq_len --max_batch_size $_max_batch_size ;
}




###
### Main
###
main_exec_profiling_with_cprofile;
main_exec_profiling_with_viztracer;




###
### Main Old History
###
# torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir llama-2-7b/  --tokenizer_path tokenizer.model --max_seq_len 128 --max_batch_size 1
# torchrun --nproc_per_node $_nproc_per_node --ckpt_dir $_ckpt_dir  --tokenizer_path tokenizer.model --max_seq_len $_max_seq_len --max_batch_size $_max_batch_size $_main_python_file   ### ERROR: this order of arguments does not work
# python -m viztracer --tracer_entries 100000000 -o .tmp.star.d/.star.prof_out.2.json torchrun --nproc_per_node 1 example_text_completion.star.py --ckpt_dir llama-2-7b/ --tokenizer_path tokenizer.model --max_seq_len 128 --max_batch_size 1




###
### Help
###
print_help()
{
	echo "[https://github.com/facebookresearch/llama Llama 2]";
	echo "[https://github.com/krychu/llama Llama 2 on CPU, and Mac M1/M2 GPU]";
	echo "[https://github.com/facebookresearch/llama/issues/436#issuecomment-1650001563 RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found? #436]";
}




