#!/bin/bash
# .bdx.0100.y.llama.main.run_and_profile.sh
# 20230910_225422
# 20230914_234326




set -x;
_ts="date +%Y%m%d_%H%M%S";
_tstamp="$($_ts)";
_tmp_bdx_d=".tmp.bdx.d";
mkdir -p $_tmp_bdx_d;




###
### Check prerequisites: execution environment (venv, torch-for-cpu)
###
echo "### Let me check where 'torchrun' is:  '$(which torchrun)'";   ### Litmus test to check the virtual environment
read -p "### Do we need to (re-)install all things required? [y|N] " _answer;
if [ "X$_answer" = "Xy" ]; then
	python -m venv .env;
	source .env/bin/activate; ### must do
	python -m pip install --upgrade pip;
	python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu;
	python -m pip install gprof2dot; ### python profiling and visualization
	python -m pip install snakeviz; ### python profiling and visualization
	python -m pip install viztracer; ### python profiling and visualization
	#python -m pip install pycscope;
	#python -m pip install pycallgraph; ### python profiling and visualization ### ERROR?
	python -m pip install -e .;
	if [ "X$(basename $(which eog))" != "Xeog" ]; then
		sudo apt install eog;
	fi
	echo $_tstamp > ${_tmp_bdx_d}/.flag.venv.created;
	echo $_tstamp > ${_tmp_bdx_d}/.flag.pkg.installed;
else
	source .env/bin/activate; ### must do
fi

###
### Check prerequisites: good download of tokenizer.model
###
if [ -f ${_tmp_bdx_d}/.flag.venv.created -a -f ${_tmp_bdx_d}/.flag.pkg.installed ]; then
	echo "### GOOD: execution environment (as a prerequisite) is ready";
	_chksum_tokenizer_ref="eeec4125e9c7560836b4873b6f8e3025";
	_chksum_tokenizer_test="$( cat tokenizer.model | md5sum | awk '{ print $1 }' )";
	if [ "X$_chksum_tokenizer_ref" = "X$_chksum_tokenizer_test" ]; then
		echo "### GOOD: tokenizer.model checksum matches the reference value";
	else
		echo "### ERROR: tokenizer.model checksum does not match -- EXIT 18";
		exit 18;
	fi
else
	echo "### ERROR: execution environment is not ready -- EXIT 18";
	exit 18;
fi




_tmp_star_d=".tmp.star.d";
mkdir -p $_tmp_star_d;




function _star_doe_conf()
{
###
### STAR DOE (Design of Experiment)
### parameters to control the test
###
#_exec_count="0";
#_xprmnt_sn="$(printf '%08d' $_exec_count)";
#_ckpt_dir="llama-2-7b/";	# "llama-2-7b/"
#_nproc_per_node="1";		# 1
#_max_seq_len="128";		# 128
#_max_gen_len="64";		# 64
#_max_batch_size="1";		# 1
#_xprmnt_ai_model="LLAMA2-7B"; ### later, include tensor (model) parallelism also
#_xprmnt_cpu_model="CORE-I7-1270P"; ### XEON-{PL/GO/SI/BR}-dddd
#_xprmnt_name="${_xprmnt_sn}_${_xprmnt_ai_model}_${_xprmnt_cpu_model}_P${_nproc_per_node}_S${_max_seq_len}_G${_max_gen_len}_B${_max_batch_size}";
#_xprmnt_uuid="${_tstamp}.${_xprmnt_name}";
	export STAR_DOE_CONF__LLAMA2__CKPT_DIR=${_ckpt_dir};
	export STAR_DOE_CONF__LLAMA2__NPROC_PER_NODE=${_nproc_per_node};
	export STAR_DOE_CONF__LLAMA2__MAX_SEQ_LEN=${_max_seq_len};
	export STAR_DOE_CONF__LLAMA2__MAX_GEN_LEN=${_max_gen_len};
	export STAR_DOE_CONF__LLAMA2__MAX_BATCH_SIZE=${_max_batch_size};
	export STAR_DOE_CONF__XPRMNT_TIMESTAMP="${_tstamp}";
	export STAR_DOE_CONF__XPRMNT_NAME="${_xprmnt_name}";
	export STAR_DOE_CONF__XPRMNT_UUID="${_xprmnt_uuid}";
###
### STAR profiling configuration
### https://docs.python.org/3/library/profile.html
### https://medium.com/@narenandu/profiling-and-visualization-tools-in-python-89a46f578989
### https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html
###
	_star_prof_out_bin=".star.${_xprmnt_uuid}.prof_out.bin"; # generated by (prof).dump_stats(f.pstats) or python -m cProfile -o f.pstats test.py
	_star_prof_out_txt=".star.${_xprmnt_uuid}.prof_out.txt"; # generated by (pstats).print_stats()
	_star_prof_out_svg=".star.${_xprmnt_uuid}.prof_out.svg"; # generated by gprof2dot
	_viztr_prof_out_json=".star.${_xprmnt_uuid}.viztracer_out.json"; # generated by viztracer
	_viztr_tracer_entries="100000000";
}

function main_exec_profiling_with_cprofile()
{
	echo "### =============================================";
	echo "### function main_exec_profiling_with_cprofile()"; 
	_main_python_file_with_cprofile="example_text_completion.star.cprofile.py";   ### _main_python_file="example_text_completion.py";
	torchrun --nproc_per_node $_nproc_per_node $_main_python_file_with_cprofile --ckpt_dir $_ckpt_dir  --tokenizer_path tokenizer.model --max_seq_len $_max_seq_len --max_batch_size $_max_batch_size ;
	( cd $_tmp_star_d; 
	gprof2dot -f pstats $_star_prof_out_bin | dot -Tsvg -o $_star_prof_out_svg;
	eog $_star_prof_out_svg &
	snakeviz $_star_prof_out_bin &
	)
}

function main_exec_profiling_with_viztracer()
{
	echo "### =============================================";
	echo "### function main_exec_profiling_with_viztracer()"; 
	_main_python_file_with_viztracer="example_text_completion.star.viztracer.py";   ### _main_python_file="example_text_completion.py";
	# python -m viztracer --tracer_entries $_viztr_tracer_entries -o ${_tmp_star_d}/${_viztr_prof_out_json} \
	torchrun --nproc_per_node $_nproc_per_node $_main_python_file_with_viztracer --ckpt_dir $_ckpt_dir  --tokenizer_path tokenizer.model --max_seq_len $_max_seq_len --max_batch_size $_max_batch_size ;
}

function _star_doe_exec()
{
#	main_exec_profiling_with_cprofile;
#	main_exec_profiling_with_viztracer;
	python star.doe.main.dummy_test.py;
}




declare -A _ai_models;
_ai_models["llama-2-13b"]="LLAMA2-13B";
_ai_models["llama-2-13b-chat"]="LLAMA2-13B-CHAT";
_ai_models["llama-2-70b"]="LLAMA2-70B";
_ai_models["llama-2-70b-chat"]="LLAMA2-70B-CHAT";
_ai_models["llama-2-7b"]="LLAMA2-7B";
_ai_models["llama-2-7b-chat"]="LLAMA2-7B-CHAT";
#
# full possible list v1
	###_list__ckpt_dir="llama-2-13b llama-2-13b-chat llama-2-70b llama-2-70b-chat llama-2-7b llama-2-7b-chat";
	###_list__nproc_per_node="1 2 4 8 16 32";
	###_list__max_seq_len="64 128 256 512 1024";
	###_list__max_gen_len="64 128 256 512 1024";
	###_list__max_batch_size="1 2 4 8 16 32";
	###_list__xprmnt_cpu_model="CORE-I7-1270P";
#
# full possible list v2
	###_list__ckpt_dir="llama-2-7b llama-2-13b";
	###_list__nproc_per_node="1 4 16 64";
	###_list__max_seq_len="16 64 256 1024";
	###_list__max_gen_len="16 64 256 1024";
	###_list__max_batch_size="1 4 16 64";
	###_list__xprmnt_cpu_model="CORE-I7-1270P";
#
# actual test cases
_list__ckpt_dir="llama-2-7b";
_list__nproc_per_node="1 2 4";
_list__max_seq_len="128";
_list__max_gen_len="128";
_list__max_batch_size="1 2 4";
_list__xprmnt_cpu_model="CORE-I7-1270P";
#
set +x;
_exec_count="0";
for _ckpt_dir in $_list__ckpt_dir; do
	_xprmnt_ai_model="${_ai_models[$_ckpt_dir]}";
	for _nproc_per_node in $_list__nproc_per_node; do
		for _max_seq_len in $_list__max_seq_len; do
			for _max_gen_len in $_list__max_gen_len; do
				for _max_batch_size in $_list__max_batch_size; do
					for _xprmnt_cpu_model in $_list__xprmnt_cpu_model; do
						_exec_count=$(expr $_exec_count + 1)
						_xprmnt_sn="$(printf '%08d' $_exec_count)";
						_xprmnt_name="${_xprmnt_sn}_${_xprmnt_ai_model}_${_xprmnt_cpu_model}_P${_nproc_per_node}_S${_max_seq_len}_G${_max_gen_len}_B${_max_batch_size}";
						_xprmnt_uuid="${_tstamp}.${_xprmnt_name}";
						echo "________________________________________________________________________________________________";
						echo $_xprmnt_uuid;
						_star_doe_conf;
						_star_doe_exec;
					done
				done
			done
		done
	done
done




###
### Main
###
#main_exec_profiling_with_cprofile;
#main_exec_profiling_with_viztracer;




###
### Main Old History
###
# torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir llama-2-7b/  --tokenizer_path tokenizer.model --max_seq_len 128 --max_batch_size 1
# torchrun --nproc_per_node $_nproc_per_node --ckpt_dir $_ckpt_dir  --tokenizer_path tokenizer.model --max_seq_len $_max_seq_len --max_batch_size $_max_batch_size $_main_python_file   ### ERROR: this order of arguments does not work
# python -m viztracer --tracer_entries 100000000 -o .tmp.star.d/.star.prof_out.2.json torchrun --nproc_per_node 1 example_text_completion.star.py --ckpt_dir llama-2-7b/ --tokenizer_path tokenizer.model --max_seq_len 128 --max_batch_size 1




###
### Help
###
print_help()
{
	echo "[https://github.com/facebookresearch/llama Llama 2]";
	echo "[https://github.com/krychu/llama Llama 2 on CPU, and Mac M1/M2 GPU]";
	echo "[https://github.com/facebookresearch/llama/issues/436#issuecomment-1650001563 RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found? #436]";
}




